{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrain the model using the complete dataset\n",
    "\n",
    "TODO: finding the best epoch should not be necessary in this stage. After fixing the tuning notebook, the steps for finding the best epoch in this notebook must be removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-14 10:42:08.977181: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-03-14 10:42:09.061488: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import layers, regularizers\n",
    "import keras_tuner as kt\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Sets off SettingWithCopyWarning.\n",
    "pd.set_option('mode.chained_assignment', None)\n",
    "\n",
    "\n",
    "# ----------------------------------------\n",
    "# Flags for working on my different machines.\n",
    "# flag_kaggle = True\n",
    "flag_FW = True\n",
    "# flag_LN = True\n",
    "\n",
    "try:\n",
    "    if flag_kaggle:\n",
    "        sys.path.insert(0, '/kaggle/input/hms-lib')\n",
    "        base_dir = '/kaggle/input/hms-harmful-brain-activity-classification'\n",
    "        devset_dir = '/kaggle/input/hms-cwt-scalograms-single-numpy-v1'\n",
    "        output_dir = ''\n",
    "except:\n",
    "    pass \n",
    "\n",
    "try:\n",
    "    if flag_FW:\n",
    "        sys.path.insert(0, '../lib')\n",
    "        base_dir = '../../kaggle_data/hms'\n",
    "        devset_dir = '../data'\n",
    "        output_dir = 'results/'\n",
    "except:\n",
    "    pass \n",
    "\n",
    "try:\n",
    "    if flag_LN:\n",
    "        sys.path.insert(0, '../lib')\n",
    "        base_dir = '../../data/hms'\n",
    "        devset_dir = '../data'\n",
    "        output_dir = 'results/'\n",
    "except:\n",
    "    pass \n",
    "# ----------------------------------------\n",
    "\n",
    "from KLmetric import score\n",
    "\n",
    "# path_train = f'{devset_dir}/05_single_cwt_v1_train.npy'\n",
    "# path_train_items = f'{devset_dir}/05_single_cwt_v1_train_items.npy'\n",
    "# path_val = f'{devset_dir}/05_single_cwt_v1_val.npy'\n",
    "# path_val_items = f'{devset_dir}/05_single_cwt_v1_val_items.npy'\n",
    "# path_test = f'{devset_dir}/05_single_cwt_v1_test.npy'\n",
    "# path_test_items = f'{devset_dir}/05_single_cwt_v1_test_items.npy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_train = f'{devset_dir}/05_reduced_single_cwt_v1_train.npy'\n",
    "path_train_items = f'{devset_dir}/05_reduced_single_cwt_v1_train_items.npy'\n",
    "path_val = f'{devset_dir}/05_reduced_single_cwt_v1_val.npy'\n",
    "path_val_items = f'{devset_dir}/05_reduced_single_cwt_v1_val_items.npy'\n",
    "path_test = f'{devset_dir}/05_reduced_single_cwt_v1_test.npy'\n",
    "path_test_items = f'{devset_dir}/05_reduced_single_cwt_v1_test_items.npy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Data generator for training.\n",
    "#\n",
    "# coefficients of cwt's arrays\n",
    "# 5 channels (LP, RP, LT, RP, C)\n",
    "#\n",
    "\n",
    "class DataGenerator(keras.utils.Sequence):\n",
    "    'Generates data for Keras'\n",
    "    def __init__(self, path_to_items, path_to_data, batch_size=32, n_classes=6, shuffle=True):\n",
    "        ''' Initialization\n",
    "        item: [eeg_id, eeg_sub_id, idx in sgrams (1st index), target,\n",
    "        seizure_vote, lpd_vote, gpd_vote, lrda_vote,\n",
    "        grda_vote, other_vote]\n",
    "        '''\n",
    "        self.n_channels = 5\n",
    "        # self.n_freqs = 40\n",
    "\n",
    "        self.data = np.load(path_to_data)\n",
    "        self.items = np.load(path_to_items)\n",
    "        self.dim = (self.data.shape[1], self.data.shape[2])\n",
    "        self.batch_size = batch_size\n",
    "        self.len = self.data.shape[0]\n",
    "        self.n_classes = n_classes\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.ceil(self.len / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "\n",
    "        # Generate data\n",
    "        X, y = self.__data_generation(indexes)\n",
    "\n",
    "        return X, y\n",
    "\n",
    "    def get_dim(self):\n",
    "        'Dimensions for the input layer.'\n",
    "        return (self.dim[0], self.dim[1], self.n_channels)\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.indexes = np.arange(self.len)\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __data_generation(self, indexes):\n",
    "        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
    "        # Initialization\n",
    "        true_size = len(indexes)\n",
    "        X = np.empty((true_size, *self.dim, self.n_channels))\n",
    "        y = np.empty((true_size, self.n_classes), dtype=float)\n",
    "\n",
    "        # Generate data\n",
    "        for i, idx in enumerate(indexes):\n",
    "            item = self.items[idx]\n",
    "            # print(item)  # Uncomment for testing.\n",
    "            X[i,:,:,:] = self.data[np.int32(item[2]), :, :, :]\n",
    "            # Store solution\n",
    "            y[i,:] = item[-6:]\n",
    "\n",
    "        return X, y\n",
    "\n",
    "\n",
    "#\n",
    "# Test Data generator for predicting\n",
    "# \n",
    "\n",
    "class TestDataGenerator(keras.utils.Sequence):\n",
    "    'Generates data for Keras'\n",
    "    def __init__(self, path_to_items, path_to_data, batch_size=32, n_classes=6, shuffle=False):\n",
    "        ''' Initialization\n",
    "        item: [eeg_id, eeg_sub_id, idx in sgrams (1st index), target,\n",
    "        seizure_vote, lpd_vote, gpd_vote, lrda_vote,\n",
    "        grda_vote, other_vote]\n",
    "        '''\n",
    "        self.n_channels = 5\n",
    "        self.data = np.load(path_to_data)\n",
    "        self.items = np.load(path_to_items)\n",
    "        self.dim = (self.data.shape[1], self.data.shape[2])\n",
    "        self.batch_size = batch_size\n",
    "        self.len = self.data.shape[0]\n",
    "        self.n_classes = n_classes\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.ceil(self.len / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "\n",
    "        # Generate data\n",
    "        X = self.__data_generation(indexes)\n",
    "\n",
    "        return X\n",
    "\n",
    "    def get_dim(self):\n",
    "        'Dimensions for the input layer.'\n",
    "        return (self.dim[0], self.dim[1], self.n_channels)\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.indexes = np.arange(self.len)\n",
    "        # pass \n",
    "        \n",
    "    def __data_generation(self, indexes):\n",
    "        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
    "        # Initialization\n",
    "        true_size = len(indexes)\n",
    "        X = np.empty((true_size, *self.dim, self.n_channels))\n",
    "\n",
    "        # Generate data\n",
    "        for i, idx in enumerate(indexes):\n",
    "            item = self.items[idx]\n",
    "            # print(item)  # Uncomment for testing.\n",
    "            X[i,:,:,:] = self.data[np.int32(item[2]), :, :, :]\n",
    "\n",
    "        return X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observations in training set: 512\n",
      "Observations in validation set: 128\n"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "params = {\n",
    "    'batch_size': 32,\n",
    "    'n_classes': 6,\n",
    "    'shuffle': True\n",
    "    }\n",
    "\n",
    "training_generator = DataGenerator(path_train_items, path_train , **params)\n",
    "validation_generator = DataGenerator(path_val_items, path_val, **params)\n",
    "\n",
    "print(\"Observations in training set:\", training_generator.__len__()*params['batch_size'])\n",
    "print(\"Observations in validation set:\", validation_generator.__len__()*params['batch_size'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_filepath = f'{output_dir}checkpoint1.model.keras'\n",
    "model = keras.models.load_model(checkpoint_filepath)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_filepath = f'{output_dir}checkpoint1-retrain.model.keras'\n",
    "model_checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    # monitor='val_kl_divergence',  # Name in Kaggle.\n",
    "    monitor='val_kullback_leibler_divergence',  # Name in FW.\n",
    "    mode='min',\n",
    "    save_best_only=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "16/16 [==============================] - 2s 112ms/step - loss: 1.0570 - kullback_leibler_divergence: 1.0570 - val_loss: 1.3689 - val_kullback_leibler_divergence: 1.3689\n",
      "Epoch 2/20\n",
      "16/16 [==============================] - 2s 104ms/step - loss: 0.9184 - kullback_leibler_divergence: 0.9184 - val_loss: 1.4460 - val_kullback_leibler_divergence: 1.4460\n",
      "Epoch 3/20\n",
      "16/16 [==============================] - 2s 105ms/step - loss: 0.7506 - kullback_leibler_divergence: 0.7506 - val_loss: 1.4891 - val_kullback_leibler_divergence: 1.4891\n",
      "Epoch 4/20\n",
      "16/16 [==============================] - 2s 105ms/step - loss: 0.6915 - kullback_leibler_divergence: 0.6915 - val_loss: 1.6910 - val_kullback_leibler_divergence: 1.6910\n",
      "Epoch 5/20\n",
      "16/16 [==============================] - 2s 107ms/step - loss: 0.6402 - kullback_leibler_divergence: 0.6402 - val_loss: 1.7512 - val_kullback_leibler_divergence: 1.7512\n",
      "Epoch 6/20\n",
      "16/16 [==============================] - 2s 104ms/step - loss: 0.5835 - kullback_leibler_divergence: 0.5835 - val_loss: 1.6577 - val_kullback_leibler_divergence: 1.6577\n",
      "Epoch 7/20\n",
      "16/16 [==============================] - 2s 102ms/step - loss: 0.4433 - kullback_leibler_divergence: 0.4433 - val_loss: 1.6369 - val_kullback_leibler_divergence: 1.6369\n",
      "Epoch 8/20\n",
      "16/16 [==============================] - 2s 104ms/step - loss: 0.4445 - kullback_leibler_divergence: 0.4445 - val_loss: 2.4562 - val_kullback_leibler_divergence: 2.4562\n",
      "Epoch 9/20\n",
      "16/16 [==============================] - 2s 101ms/step - loss: 0.4804 - kullback_leibler_divergence: 0.4804 - val_loss: 2.4313 - val_kullback_leibler_divergence: 2.4313\n",
      "Epoch 10/20\n",
      "16/16 [==============================] - 2s 100ms/step - loss: 0.3854 - kullback_leibler_divergence: 0.3854 - val_loss: 1.9237 - val_kullback_leibler_divergence: 1.9237\n",
      "Epoch 11/20\n",
      "16/16 [==============================] - 2s 99ms/step - loss: 0.2845 - kullback_leibler_divergence: 0.2845 - val_loss: 1.9643 - val_kullback_leibler_divergence: 1.9643\n",
      "Epoch 12/20\n",
      "16/16 [==============================] - 2s 101ms/step - loss: 0.2874 - kullback_leibler_divergence: 0.2874 - val_loss: 1.8978 - val_kullback_leibler_divergence: 1.8978\n",
      "Epoch 13/20\n",
      "16/16 [==============================] - 2s 96ms/step - loss: 0.2496 - kullback_leibler_divergence: 0.2496 - val_loss: 2.0539 - val_kullback_leibler_divergence: 2.0539\n",
      "Epoch 14/20\n",
      "16/16 [==============================] - 2s 104ms/step - loss: 0.2455 - kullback_leibler_divergence: 0.2455 - val_loss: 2.1431 - val_kullback_leibler_divergence: 2.1431\n",
      "Epoch 15/20\n",
      "16/16 [==============================] - 2s 100ms/step - loss: 0.2148 - kullback_leibler_divergence: 0.2148 - val_loss: 2.0175 - val_kullback_leibler_divergence: 2.0175\n",
      "Epoch 16/20\n",
      "16/16 [==============================] - 2s 100ms/step - loss: 0.2304 - kullback_leibler_divergence: 0.2304 - val_loss: 2.0310 - val_kullback_leibler_divergence: 2.0310\n",
      "Epoch 17/20\n",
      "16/16 [==============================] - 2s 100ms/step - loss: 0.1863 - kullback_leibler_divergence: 0.1863 - val_loss: 2.6318 - val_kullback_leibler_divergence: 2.6318\n",
      "Epoch 18/20\n",
      "16/16 [==============================] - 2s 99ms/step - loss: 0.2297 - kullback_leibler_divergence: 0.2297 - val_loss: 2.0728 - val_kullback_leibler_divergence: 2.0728\n",
      "Epoch 19/20\n",
      "16/16 [==============================] - 2s 102ms/step - loss: 0.1516 - kullback_leibler_divergence: 0.1516 - val_loss: 2.0856 - val_kullback_leibler_divergence: 2.0856\n",
      "Epoch 20/20\n",
      "16/16 [==============================] - 2s 100ms/step - loss: 0.1509 - kullback_leibler_divergence: 0.1509 - val_loss: 2.0875 - val_kullback_leibler_divergence: 2.0875\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(training_generator, epochs=20,\n",
    "          validation_data=validation_generator,\n",
    "          callbacks=[model_checkpoint_callback])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best epoch: 1\n"
     ]
    }
   ],
   "source": [
    "val_kl_per_epoch = history.history['val_kullback_leibler_divergence']\n",
    "best_epoch = val_kl_per_epoch.index(min(val_kl_per_epoch)) + 1\n",
    "print(f'Best epoch: {best_epoch}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge training and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Data generator for training.\n",
    "#\n",
    "# coefficients of cwt's arrays\n",
    "# 5 channels (LP, RP, LT, RP, C)\n",
    "#\n",
    "\n",
    "class RetrainDataGenerator(keras.utils.Sequence):\n",
    "    'Generates data for Keras'\n",
    "    def __init__(self, path_to_train_items, path_to_train_data, path_to_val_items, path_to_val_data, batch_size=32, n_classes=6, shuffle=True):\n",
    "        ''' Initialization\n",
    "        item: [eeg_id, eeg_sub_id, idx in sgrams (1st index), target,\n",
    "        seizure_vote, lpd_vote, gpd_vote, lrda_vote,\n",
    "        grda_vote, other_vote]\n",
    "        '''\n",
    "        self.n_channels = 5\n",
    "        # self.n_freqs = 40\n",
    "\n",
    "        self.data = np.concatenate([np.load(path_to_train_data), np.load(path_to_val_data)])\n",
    "        self.items = np.concatenate([np.load(path_to_train_items), np.load(path_to_val_items)])\n",
    "        self.dim = (self.data.shape[1], self.data.shape[2])\n",
    "        self.batch_size = batch_size\n",
    "        self.len = self.data.shape[0]\n",
    "        self.n_classes = n_classes\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.ceil(self.len / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "\n",
    "        # Generate data\n",
    "        X, y = self.__data_generation(indexes)\n",
    "\n",
    "        return X, y\n",
    "\n",
    "    def get_dim(self):\n",
    "        'Dimensions for the input layer.'\n",
    "        return (self.dim[0], self.dim[1], self.n_channels)\n",
    "\n",
    "    def get_num_observations(self):\n",
    "        return self.data.shape[0]\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.indexes = np.arange(self.len)\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __data_generation(self, indexes):\n",
    "        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
    "        # Initialization\n",
    "        true_size = len(indexes)\n",
    "        X = np.empty((true_size, *self.dim, self.n_channels))\n",
    "        y = np.empty((true_size, self.n_classes), dtype=float)\n",
    "\n",
    "        # Generate data\n",
    "        for i, idx in enumerate(indexes):\n",
    "            item = self.items[idx]\n",
    "            # print(item)  # Uncomment for testing.\n",
    "            X[i,:,:,:] = self.data[np.int32(item[2]), :, :, :]\n",
    "            # Store solution\n",
    "            y[i,:] = item[-6:]\n",
    "\n",
    "        return X, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observations in training set: 600\n"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "params = {\n",
    "    'batch_size': 32,\n",
    "    'n_classes': 6,\n",
    "    'shuffle': True\n",
    "    }\n",
    "\n",
    "retraining_generator = RetrainDataGenerator(path_train_items, path_train, path_val_items, path_val, **params)\n",
    "\n",
    "print(\"Observations in training set:\", retraining_generator.get_num_observations())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "19/19 [==============================] - 2s 90ms/step - loss: 1.0332 - kullback_leibler_divergence: 1.0332\n",
      "Epoch 2/5\n",
      "19/19 [==============================] - 2s 92ms/step - loss: 0.9766 - kullback_leibler_divergence: 0.9766\n",
      "Epoch 3/5\n",
      "19/19 [==============================] - 2s 93ms/step - loss: 0.8576 - kullback_leibler_divergence: 0.8576\n",
      "Epoch 4/5\n",
      "19/19 [==============================] - 2s 91ms/step - loss: 0.7813 - kullback_leibler_divergence: 0.7813\n",
      "Epoch 5/5\n",
      "19/19 [==============================] - 2s 89ms/step - loss: 0.7325 - kullback_leibler_divergence: 0.7325\n"
     ]
    }
   ],
   "source": [
    "checkpoint_filepath = f'{output_dir}checkpoint1-retrain.model.keras'\n",
    "model = keras.models.load_model(checkpoint_filepath)\n",
    "\n",
    "history = model.fit(retraining_generator, epochs=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_filename = f'{output_dir}hms-keras-12-cwt-reduced.keras'\n",
    "model.save(model_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 21ms/step\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    'batch_size': 32,\n",
    "    'n_classes': 6,\n",
    "    }\n",
    "\n",
    "test_generator = TestDataGenerator(path_test_items, path_test, **params)\n",
    "\n",
    "y_pred = model.predict(test_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.7986183592759002"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TARGETS = ['seizure_vote', 'lpd_vote', 'gpd_vote', 'lrda_vote', 'grda_vote', 'other_vote']\n",
    "\n",
    "test_items = np.load(path_test_items)\n",
    "df_test_items = pd.DataFrame(test_items)\n",
    "df_test_items[0] = df_test_items[0].astype(int)\n",
    "\n",
    "sub = pd.DataFrame({'eeg_id':df_test_items[0]})\n",
    "sub[TARGETS] = np.round(y_pred,6)\n",
    "# sub.to_csv('submission.csv',index=False)\n",
    "\n",
    "df_test_scoring = df_test_items[[0,4,5,6,7,8,9]]\n",
    "df_test_scoring.columns = sub.columns\n",
    "\n",
    "score(df_test_scoring, sub, 'eeg_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gsdc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
