{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00db8aa9",
   "metadata": {
    "papermill": {
     "duration": 0.004896,
     "end_time": "2024-02-19T18:30:53.912637",
     "exception": false,
     "start_time": "2024-02-19T18:30:53.907741",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# CWT scalograms\n",
    "\n",
    "Building model using a reduced dataset and load it for test set predictions in Kaggle.\n",
    "\n",
    "5 channels (LT, RT, LP, RP, C).\n",
    "\n",
    "Implementing tf.keras.metrics.KLDivergence().\n",
    "\n",
    "- Training run.\n",
    "- Saving model and checkpoint.\n",
    "- Inspecting: loading model and checkpoint.\n",
    "- Preprocessing of test eegs.\n",
    "- Predictions and submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "91ea511e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-19T18:30:53.923784Z",
     "iopub.status.busy": "2024-02-19T18:30:53.923433Z",
     "iopub.status.idle": "2024-02-19T18:31:06.918431Z",
     "shell.execute_reply": "2024-02-19T18:31:06.917331Z"
    },
    "papermill": {
     "duration": 13.003552,
     "end_time": "2024-02-19T18:31:06.921234",
     "exception": false,
     "start_time": "2024-02-19T18:30:53.917682",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-10 16:20:45.509572: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-03-10 16:20:45.596400: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import regularizers\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "base_dir = '../../kaggle_data/hms'\n",
    "# base_dir = '../../data/hms'\n",
    "# base_dir = '/kaggle/input/hms-harmful-brain-activity-classification'\n",
    "\n",
    "devset_dir = '../data'\n",
    "# devset_dir = '/kaggle/input/hms-single-spectrograms-v1'\n",
    "\n",
    "# path_train = f'{devset_dir}/05_single_cwt_v1_train.npy'\n",
    "# path_train_items = f'{devset_dir}/05_single_cwt_v1_train_items.npy'\n",
    "# path_val = f'{devset_dir}/05_single_cwt_v1_val.npy'\n",
    "# path_val_items = f'{devset_dir}/05_single_cwt_v1_val_items.npy'\n",
    "# path_test = f'{devset_dir}/05_single_cwt_v1_test.npy'\n",
    "# path_test_items = f'{devset_dir}/05_single_cwt_v1_test_items.npy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33ca8df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_train = f'{devset_dir}/05_reduced_single_cwt_v1_train.npy'\n",
    "path_train_items = f'{devset_dir}/05_reduced_single_cwt_v1_train_items.npy'\n",
    "path_val = f'{devset_dir}/05_reduced_single_cwt_v1_val.npy'\n",
    "path_val_items = f'{devset_dir}/05_reduced_single_cwt_v1_val_items.npy'\n",
    "path_test = f'{devset_dir}/05_reduced_single_cwt_v1_test.npy'\n",
    "path_test_items = f'{devset_dir}/05_reduced_single_cwt_v1_test_items.npy'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d23428eb",
   "metadata": {
    "papermill": {
     "duration": 0.005501,
     "end_time": "2024-02-19T18:31:06.932688",
     "exception": false,
     "start_time": "2024-02-19T18:31:06.927187",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Definitions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "635a67de",
   "metadata": {
    "papermill": {
     "duration": 0.004653,
     "end_time": "2024-02-19T18:31:07.002231",
     "exception": false,
     "start_time": "2024-02-19T18:31:06.997578",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "94c9a9f4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-19T18:31:07.013190Z",
     "iopub.status.busy": "2024-02-19T18:31:07.012628Z",
     "iopub.status.idle": "2024-02-19T18:31:07.025580Z",
     "shell.execute_reply": "2024-02-19T18:31:07.024843Z"
    },
    "papermill": {
     "duration": 0.020601,
     "end_time": "2024-02-19T18:31:07.027552",
     "exception": false,
     "start_time": "2024-02-19T18:31:07.006951",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Data generator using numpy and no pandas.\n",
    "#\n",
    "# scalograms\n",
    "# 30 seconds slice (I think)\n",
    "# 5 channels (LP, RP, LT, RP, C)\n",
    "#\n",
    "\n",
    "class DataGenerator(keras.utils.Sequence):\n",
    "    'Generates data for Keras'\n",
    "    def __init__(self, path_to_items, path_to_data, batch_size=32, n_classes=6, shuffle=True):\n",
    "        ''' Initialization\n",
    "        item: [eeg_id, eeg_sub_id, idx in sgrams (1st index), target,\n",
    "        seizure_vote, lpd_vote, gpd_vote, lrda_vote,\n",
    "        grda_vote, other_vote]\n",
    "        '''\n",
    "        self.n_channels = 5\n",
    "        # self.n_freqs = 40\n",
    "\n",
    "        self.data = np.load(path_to_data)\n",
    "        self.items = np.load(path_to_items)\n",
    "        self.dim = (self.data.shape[1], self.data.shape[2])\n",
    "        self.batch_size = batch_size\n",
    "        self.len = self.data.shape[0]\n",
    "        self.n_classes = n_classes\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.ceil(self.len / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "\n",
    "        # Generate data\n",
    "        X, y = self.__data_generation(indexes)\n",
    "\n",
    "        return X, y\n",
    "\n",
    "    def get_dim(self):\n",
    "        'Dimensions for the input layer.'\n",
    "        return (self.dim[0], self.dim[1], self.n_channels)\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.indexes = np.arange(self.len)\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __data_generation(self, indexes):\n",
    "        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
    "        # Initialization\n",
    "        true_size = len(indexes)\n",
    "        X = np.empty((true_size, *self.dim, self.n_channels))\n",
    "        y = np.empty((true_size, self.n_classes), dtype=float)\n",
    "\n",
    "        # Generate data\n",
    "        for i, idx in enumerate(indexes):\n",
    "            item = self.items[idx]\n",
    "            # print(item)  # Uncomment for testing.\n",
    "            X[i,:,:,:] = self.data[np.int32(item[2]), :, :, :]\n",
    "            # Store solution\n",
    "            y[i,:] = item[-6:]\n",
    "\n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d42b19af",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-19T18:31:07.038776Z",
     "iopub.status.busy": "2024-02-19T18:31:07.038455Z",
     "iopub.status.idle": "2024-02-19T18:31:07.049272Z",
     "shell.execute_reply": "2024-02-19T18:31:07.048499Z"
    },
    "papermill": {
     "duration": 0.018651,
     "end_time": "2024-02-19T18:31:07.051182",
     "exception": false,
     "start_time": "2024-02-19T18:31:07.032531",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def make_model(input_shape, num_classes):\n",
    "    input_layer = keras.layers.Input(input_shape)\n",
    "\n",
    "    #max1 = keras.layers.MaxPooling1D(pool_size=2)(input_layer)\n",
    "    \n",
    "    conv1 = keras.layers.Conv2D(filters=32, kernel_size=3, padding=\"same\")(input_layer)\n",
    "    conv1 = keras.layers.BatchNormalization()(conv1)\n",
    "    conv1 = keras.layers.MaxPooling2D(pool_size=4)(conv1)\n",
    "    conv1 = keras.layers.ReLU()(conv1)\n",
    "    \n",
    "    # conv2 = keras.layers.Conv2D(filters=64, kernel_size=7, padding=\"same\")(conv1)\n",
    "    # #conv2 = keras.layers.BatchNormalization()(conv2)\n",
    "    # # conv2 = keras.layers.MaxPooling2D(pool_size=8)(conv2)\n",
    "    # conv2 = keras.layers.ReLU()(conv2)\n",
    "\n",
    "    # conv3 = keras.layers.Conv2D(filters=256, kernel_size=7, padding=\"same\")(conv2)\n",
    "    # #conv3 = keras.layers.BatchNormalization()(conv3)\n",
    "    # conv3 = keras.layers.MaxPooling2D(pool_size=2)(conv3)\n",
    "    # conv3 = keras.layers.ReLU()(conv3)\n",
    "\n",
    "    # conv4 = keras.layers.Conv2D(filters=512, kernel_size=3, padding=\"same\")(conv3)\n",
    "    # conv4 = keras.layers.BatchNormalization()(conv4)\n",
    "    # conv4 = keras.layers.MaxPooling2D(pool_size=4)(conv4)\n",
    "    # conv4 = keras.layers.ReLU()(conv4)\n",
    "\n",
    "    fltn  = keras.layers.Flatten()(conv1) \n",
    "    \n",
    "    relu1 = keras.layers.Dense(64)(fltn)\n",
    "    relu1 = keras.layers.ReLU()(relu1)\n",
    "\n",
    "    # relu2 = keras.layers.Dense(64)(relu1)\n",
    "    # relu2 = keras.layers.ReLU(64)(relu2)\n",
    "\n",
    "#     lin = keras.layers.Dense(2)(relu2)\n",
    "\n",
    "    output_layer = keras.layers.Dense(num_classes, activation=\"softmax\")(relu1)\n",
    "\n",
    "    return keras.models.Model(inputs=input_layer, outputs=output_layer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e9fa624a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-19T18:31:07.062937Z",
     "iopub.status.busy": "2024-02-19T18:31:07.062324Z",
     "iopub.status.idle": "2024-02-19T18:31:40.020998Z",
     "shell.execute_reply": "2024-02-19T18:31:40.020003Z"
    },
    "papermill": {
     "duration": 32.97052,
     "end_time": "2024-02-19T18:31:40.026924",
     "exception": false,
     "start_time": "2024-02-19T18:31:07.056404",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observations in training set: 512\n",
      "Observations in validation set: 128\n"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "params = {\n",
    "    'batch_size': 32,\n",
    "    'n_classes': 6,\n",
    "    'shuffle': True\n",
    "    }\n",
    "\n",
    "training_generator = DataGenerator(path_train_items, path_train , **params)\n",
    "validation_generator = DataGenerator(path_val_items, path_val, **params)\n",
    "\n",
    "print(\"Observations in training set:\", training_generator.__len__()*params['batch_size'])\n",
    "print(\"Observations in validation set:\", validation_generator.__len__()*params['batch_size'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6b5b34bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Kaggle version\n",
    "# # Name to monitor is different.\n",
    "# checkpoint_filepath = 'results/checkpoint1.model.keras'\n",
    "# model_checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
    "#     filepath=checkpoint_filepath,\n",
    "#     monitor='val_kl_divergence',\n",
    "#     mode='min',\n",
    "#     save_best_only=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "105e83cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name to monitor is different.\n",
    "checkpoint_filepath = 'results/checkpoint1.model.keras'\n",
    "model_checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    monitor='val_kullback_leibler_divergence',\n",
    "    mode='min',\n",
    "    save_best_only=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3cda2ab3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "16/16 [==============================] - 3s 181ms/step - loss: 11.5522 - kullback_leibler_divergence: 11.5522 - val_loss: 4.4372 - val_kullback_leibler_divergence: 4.4372\n",
      "Epoch 2/10\n",
      "16/16 [==============================] - 3s 175ms/step - loss: 4.3561 - kullback_leibler_divergence: 4.3561 - val_loss: 1.4256 - val_kullback_leibler_divergence: 1.4256\n",
      "Epoch 3/10\n",
      "16/16 [==============================] - 3s 178ms/step - loss: 1.4083 - kullback_leibler_divergence: 1.4083 - val_loss: 1.4109 - val_kullback_leibler_divergence: 1.4109\n",
      "Epoch 4/10\n",
      "16/16 [==============================] - 3s 171ms/step - loss: 1.3545 - kullback_leibler_divergence: 1.3545 - val_loss: 1.4067 - val_kullback_leibler_divergence: 1.4067\n",
      "Epoch 5/10\n",
      "16/16 [==============================] - 3s 176ms/step - loss: 1.3115 - kullback_leibler_divergence: 1.3115 - val_loss: 1.4000 - val_kullback_leibler_divergence: 1.4000\n",
      "Epoch 6/10\n",
      "16/16 [==============================] - 3s 168ms/step - loss: 1.3335 - kullback_leibler_divergence: 1.3335 - val_loss: 1.4146 - val_kullback_leibler_divergence: 1.4146\n",
      "Epoch 7/10\n",
      "16/16 [==============================] - 3s 183ms/step - loss: 1.2953 - kullback_leibler_divergence: 1.2953 - val_loss: 1.4345 - val_kullback_leibler_divergence: 1.4345\n",
      "Epoch 8/10\n",
      "16/16 [==============================] - 3s 172ms/step - loss: 1.2594 - kullback_leibler_divergence: 1.2594 - val_loss: 1.4417 - val_kullback_leibler_divergence: 1.4417\n",
      "Epoch 9/10\n",
      "16/16 [==============================] - 3s 173ms/step - loss: 1.2502 - kullback_leibler_divergence: 1.2502 - val_loss: 1.5440 - val_kullback_leibler_divergence: 1.5440\n",
      "Epoch 10/10\n",
      "16/16 [==============================] - 3s 171ms/step - loss: 1.3236 - kullback_leibler_divergence: 1.3236 - val_loss: 1.4265 - val_kullback_leibler_divergence: 1.4265\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x70689e16fd60>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt = keras.optimizers.SGD(\n",
    "    learning_rate=0.01,\n",
    "    momentum=0.01,\n",
    ")\n",
    "\n",
    "# opt = keras.optimizers.Adam(\n",
    "#     learning_rate=0.004,\n",
    "# )\n",
    "\n",
    "dim = training_generator.get_dim()\n",
    "\n",
    "model = make_model(input_shape=dim, num_classes=6)\n",
    "\n",
    "# model.load_weights('/kaggle/input/hms-model-cwt-v1/checkpoint.model.keras')\n",
    "\n",
    "model.compile(optimizer=opt,\n",
    "            loss=tf.keras.losses.KLDivergence(),\n",
    "            metrics=[tf.keras.metrics.KLDivergence()])\n",
    "\n",
    "model.fit(training_generator, epochs=10,\n",
    "          validation_data=validation_generator,\n",
    "          callbacks=[model_checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "90f5dbfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"results/hms-keras-10-model-reduced.keras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1b0e74b",
   "metadata": {},
   "source": [
    "## Inspecting model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "cc0d15f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = keras.models.load_model(\"results/hms-keras-10-model-reduced.keras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f24e5b9",
   "metadata": {
    "papermill": {
     "duration": 0.106847,
     "end_time": "2024-02-19T18:42:03.442617",
     "exception": false,
     "start_time": "2024-02-19T18:42:03.335770",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "983ea1f8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-19T18:42:03.659619Z",
     "iopub.status.busy": "2024-02-19T18:42:03.658863Z",
     "iopub.status.idle": "2024-02-19T18:42:17.517770Z",
     "shell.execute_reply": "2024-02-19T18:42:17.516736Z"
    },
    "papermill": {
     "duration": 13.970827,
     "end_time": "2024-02-19T18:42:17.520255",
     "exception": false,
     "start_time": "2024-02-19T18:42:03.549428",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "TARGETS = ['seizure_vote', 'lpd_vote', 'gpd_vote', 'lrda_vote', 'grda_vote', 'other_vote']\n",
    "\n",
    "#\n",
    "# Test Data generator: for predicting\n",
    "# using own test set.\n",
    "# (Not for predicting LB)\n",
    "#\n",
    "\n",
    "class TestDataGenerator(keras.utils.Sequence):\n",
    "    'Generates data for Keras'\n",
    "    def __init__(self, path_to_items, path_to_data, batch_size=32, n_classes=6, shuffle=False):\n",
    "        ''' Initialization\n",
    "        items: [eeg_id, eeg_sub_id, idx of offset, target, ...]\n",
    "        '''\n",
    "        self.n_channels = 5\n",
    "        self.data = np.load(path_to_data)\n",
    "        self.items = np.load(path_to_items)\n",
    "        self.dim = (self.data.shape[1], self.data.shape[2])\n",
    "        self.batch_size = batch_size\n",
    "        self.len = self.data.shape[0]\n",
    "        self.n_classes = n_classes\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.ceil(self.len / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "\n",
    "        # Generate data\n",
    "        X = self.__data_generation(indexes)\n",
    "\n",
    "        return X\n",
    "\n",
    "    def get_dim(self):\n",
    "        'Dimensions for the input layer.'\n",
    "        return (self.dim[0], self.dim[1], self.n_channels)\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.indexes = np.arange(self.len)\n",
    "        # pass \n",
    "        \n",
    "    def __data_generation(self, indexes):\n",
    "        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
    "        # Initialization\n",
    "        true_size = len(indexes)\n",
    "        X = np.empty((true_size, *self.dim, self.n_channels))\n",
    "\n",
    "        # Generate data\n",
    "        for i, idx in enumerate(indexes):\n",
    "            item = self.items[idx]\n",
    "            # print(item)  # Uncomment for testing.\n",
    "            X[i,:,:,:] = self.data[np.int32(item[2]), :, :, :]\n",
    "\n",
    "        return X\n",
    "    \n",
    "                \n",
    "params = {\n",
    "    'batch_size': 32,\n",
    "    'n_classes': 6,\n",
    "    }\n",
    "\n",
    "test_generator = TestDataGenerator(path_test_items, path_test, **params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "3f706584",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 39ms/step\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(test_generator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7392517d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 40ms/step\n"
     ]
    }
   ],
   "source": [
    "y_pred_new = new_model.predict(test_generator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "7680c1a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.all(y_pred == y_pred_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c09764",
   "metadata": {},
   "source": [
    "Loading the model produce the same predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b56bfded",
   "metadata": {},
   "source": [
    "### Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "0924185b",
   "metadata": {},
   "outputs": [],
   "source": [
    "chk_model = make_model(input_shape=dim, num_classes=6)\n",
    "\n",
    "chk_model.load_weights(checkpoint_filepath)\n",
    "\n",
    "chk_model.compile(optimizer=opt,\n",
    "            loss=tf.keras.losses.KLDivergence(),\n",
    "            metrics=[tf.keras.metrics.KLDivergence()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a520f9d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 42ms/step\n"
     ]
    }
   ],
   "source": [
    "y_pred_chk = chk_model.predict(test_generator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c4fa9a1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.all(y_pred == y_pred_chk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f86071a4",
   "metadata": {},
   "source": [
    "Loading model, then loading weights of checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "8fb930c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model.load_weights(checkpoint_filepath)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "aded9a3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 38ms/step\n"
     ]
    }
   ],
   "source": [
    "y_pred_new_and_chkp = new_model.predict(test_generator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "28367a8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.all(y_pred_chk == y_pred_new_and_chkp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a38c23",
   "metadata": {},
   "source": [
    "Same predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "10891b08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_6 (InputLayer)        [(None, 49, 400, 5)]      0         \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 49, 400, 32)       1472      \n",
      "                                                                 \n",
      " batch_normalization_5 (Bat  (None, 49, 400, 32)       128       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling2d_5 (MaxPoolin  (None, 12, 100, 32)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " re_lu_10 (ReLU)             (None, 12, 100, 32)       0         \n",
      "                                                                 \n",
      " flatten_5 (Flatten)         (None, 38400)             0         \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 64)                2457664   \n",
      "                                                                 \n",
      " re_lu_11 (ReLU)             (None, 64)                0         \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 6)                 390       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2459654 (9.38 MB)\n",
      "Trainable params: 2459590 (9.38 MB)\n",
      "Non-trainable params: 64 (256.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "chk_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6bac7ee",
   "metadata": {},
   "source": [
    "## Test eegs preprocessing for Kaggle\n",
    "\n",
    "Things to change for Kaggle:\n",
    "\n",
    "1. Folder for libs.\n",
    "1. Folder for test_eegs\n",
    "1. Interpolate?\n",
    "1. Remove timing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "fd7881c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "292"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pywt\n",
    "import time\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '../lib')\n",
    "from lib_banana import banana\n",
    "from lib_pooling import poolingOverlap\n",
    "\n",
    "\n",
    "test_path = '../toy_data/test_eegs'\n",
    "\n",
    "test_files = os.listdir(test_path)\n",
    "test_size = len(test_files)\n",
    "test_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d184dd7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "scales = np.arange(1,50)\n",
    "waveletname = 'morl'\n",
    "n_channels = 5\n",
    "dim1 = scales.shape[0]\n",
    "pool_window = 5\n",
    "dim2 = int(2000/pool_window)\n",
    "sampling_period = 1\n",
    "# Center 10 s adjusted by pooling window.\n",
    "start2 = int(4000/pool_window)\n",
    "end2 = int(6000/pool_window)\n",
    "\n",
    "sgrams = np.empty((test_size, dim1, dim2, n_channels))\n",
    "# item: [eeg_id, eeg_sub_id, idx in sgrams (1st index), target,\n",
    "#       seizure_vote, lpd_vote, gpd_vote, lrda_vote,\n",
    "#       grda_vote, other_vote]\n",
    "items = np.array([], dtype=float).reshape(0,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "bf417426",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for preprocessing 292 files: 78.77589142999932 s.\n"
     ]
    }
   ],
   "source": [
    "t1 = time.perf_counter()\n",
    "\n",
    "for i, file in enumerate(test_files):\n",
    "    eeg_full = pd.read_parquet(f'{test_path}/{file}')\n",
    "    # eeg_full = eeg_full.interpolate(limit_direction='both') # <<<<< Interpolation\n",
    "    eeg = banana(eeg_full, filter=False)\n",
    "\n",
    "        # Averaging each chain in the banana montage.\n",
    "\n",
    "    # Left temporal chain.\n",
    "    coeff = np.zeros((dim1, 10000))\n",
    "    # coeff = np.zeros((dim1, 6000))  # keeping 30 s to reduce runtime.\n",
    "    for col in [0,1,2,3]:\n",
    "        coeff_, freq = pywt.cwt(eeg.iloc[:,col], scales, waveletname, sampling_period=sampling_period)\n",
    "        coeff = coeff + coeff_\n",
    "\n",
    "    coeff = coeff/4\n",
    "    coeff = poolingOverlap(coeff,(1,pool_window),stride=None,method='mean',pad=False)\n",
    "    coeff = (coeff - np.mean(coeff)) / np.std(coeff)\n",
    "    sgrams[i,:,:,0] = coeff[:,start2:end2].copy()\n",
    "\n",
    "    # Right temporal chain.\n",
    "    coeff = np.zeros((dim1, 10000))\n",
    "    for col in [4,5,6,7]:\n",
    "        coeff_, freq = pywt.cwt(eeg.iloc[:,col], scales, waveletname, sampling_period=sampling_period)\n",
    "        coeff = coeff + coeff_\n",
    "\n",
    "    coeff = coeff/4\n",
    "    coeff = poolingOverlap(coeff,(1,pool_window),stride=None,method='mean',pad=False)\n",
    "    coeff = (coeff - np.mean(coeff)) / np.std(coeff)\n",
    "    sgrams[i,:,:,1] = coeff[:,start2:end2].copy()\n",
    "\n",
    "    # Left parasagittal chain.\n",
    "    coeff = np.zeros((dim1, 10000))\n",
    "    for col in [8,9,10,11]:\n",
    "        coeff_, freq = pywt.cwt(eeg.iloc[:,col], scales, waveletname, sampling_period=sampling_period)\n",
    "        coeff = coeff + coeff_\n",
    "\n",
    "    coeff = coeff/4\n",
    "    coeff = poolingOverlap(coeff,(1,pool_window),stride=None,method='mean',pad=False)\n",
    "    coeff = (coeff - np.mean(coeff)) / np.std(coeff)\n",
    "    sgrams[i,:,:,2] = coeff[:,start2:end2].copy()\n",
    "\n",
    "    # Right parasagittal chain.\n",
    "    coeff = np.zeros((dim1, 10000))\n",
    "    for col in [12,13,14,15]:\n",
    "        coeff_, freq = pywt.cwt(eeg.iloc[:,col], scales, waveletname, sampling_period=sampling_period)\n",
    "        coeff = coeff + coeff_\n",
    "\n",
    "    coeff = coeff/4\n",
    "    coeff = poolingOverlap(coeff,(1,pool_window),stride=None,method='mean',pad=False)\n",
    "    coeff = (coeff - np.mean(coeff)) / np.std(coeff)\n",
    "    sgrams[i,:,:,3] = coeff[:,start2:end2].copy()\n",
    "\n",
    "    # Central chain.\n",
    "    coeff = np.zeros((dim1, 10000))\n",
    "    for col in [16,17]:\n",
    "        coeff_, freq = pywt.cwt(eeg.iloc[:,col], scales, waveletname, sampling_period=sampling_period)\n",
    "        coeff = coeff + coeff_\n",
    "\n",
    "    coeff = coeff/2\n",
    "    coeff = poolingOverlap(coeff,(1,pool_window),stride=None,method='mean',pad=False)\n",
    "    coeff = (coeff - np.mean(coeff)) / np.std(coeff)\n",
    "    sgrams[i,:,:,4] = coeff[:,start2:end2].copy()\n",
    "\n",
    "    eeg_id = int(test_files[2].split('.')[0])\n",
    "    # Set unkowns to zero, to reuse code.\n",
    "    xitem = np.array([eeg_id, 0, i, 0, 0, 0, 0,\n",
    "                      0, 0, 0], dtype=float).reshape(1,10)\n",
    "    items = np.concatenate([items, xitem])\n",
    "\n",
    "t2 = time.perf_counter()\n",
    "print(f'Time for preprocessing {test_size} files: {np.round(t2-t1,3)} s.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "42187b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = time.perf_counter()\n",
    "\n",
    "TARGETS = ['seizure_vote', 'lpd_vote', 'gpd_vote', 'lrda_vote', 'grda_vote', 'other_vote']\n",
    "\n",
    "#\n",
    "# Test Data generator\n",
    "#\n",
    "# for predictions in Kaggle.\n",
    "# \n",
    "\n",
    "class TestDataGenerator(keras.utils.Sequence):\n",
    "    'Generates data for Keras'\n",
    "    def __init__(self, items, data, batch_size=32, n_classes=6, shuffle=False):\n",
    "        ''' Initialization\n",
    "        items: [eeg_id, eeg_sub_id, idx of offset, target, ...]\n",
    "        '''\n",
    "        self.n_channels = 5\n",
    "        self.data = data\n",
    "        self.items = items\n",
    "        self.dim = (self.data.shape[1], self.data.shape[2])\n",
    "        self.batch_size = batch_size\n",
    "        self.len = self.data.shape[0]\n",
    "        self.n_classes = n_classes\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.ceil(self.len / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "\n",
    "        # Generate data\n",
    "        X = self.__data_generation(indexes)\n",
    "\n",
    "        return X\n",
    "\n",
    "    def get_dim(self):\n",
    "        'Dimensions for the input layer.'\n",
    "        return (self.dim[0], self.dim[1], self.n_channels)\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.indexes = np.arange(self.len)\n",
    "        # pass \n",
    "        \n",
    "    def __data_generation(self, indexes):\n",
    "        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
    "        # Initialization\n",
    "        true_size = len(indexes)\n",
    "        X = np.empty((true_size, *self.dim, self.n_channels))\n",
    "\n",
    "        # Generate data\n",
    "        for i, idx in enumerate(indexes):\n",
    "            item = self.items[idx]\n",
    "            # print(item)  # Uncomment for testing.\n",
    "            X[i,:,:,:] = self.data[np.int32(item[2]), :, :, :]\n",
    "\n",
    "        return X\n",
    "\n",
    "params = {\n",
    "    'batch_size': 32,\n",
    "    'n_classes': 6,\n",
    "    }\n",
    "\n",
    "test_generator = TestDataGenerator(items, sgrams, **params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da716ee3",
   "metadata": {},
   "source": [
    "Loading the model and predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "032a80a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 39ms/step\n"
     ]
    }
   ],
   "source": [
    "loaded_model = keras.models.load_model(\"results/hms-keras-10-model-reduced.keras\")\n",
    "loaded_model.load_weights(checkpoint_filepath)\n",
    "y_pred = loaded_model.predict(test_generator)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "d32689ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for predicting 292 files: 782.806 s.\n"
     ]
    }
   ],
   "source": [
    "t2 = time.perf_counter()\n",
    "print(f'Time for predicting {test_size} files: {np.round(t2-t1,3)} s.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00570edc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 7469972,
     "sourceId": 59093,
     "sourceType": "competition"
    },
    {
     "datasetId": 4432380,
     "sourceId": 7611741,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4461201,
     "sourceId": 7652435,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30648,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 689.517384,
   "end_time": "2024-02-19T18:42:20.635927",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-02-19T18:30:51.118543",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
