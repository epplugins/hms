{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Measuring performance\n",
    "\n",
    "Testing different approaches.\n",
    "\n",
    "model.fit :\n",
    "\n",
    "| Desc       | Where       | Time  | Obs. |\n",
    "| ---        | ----        | ----  | ---- |\n",
    "| A - 1st try    | framework   |  1m 48s | |\n",
    "| A - 1st try    | Kaggle CPU  | 3m 4s  | |\n",
    "| A - 1st try    | Kaggle GPU  | 27.4s  | Get batch takes most of the time. |\n",
    "| B - Only numpy | Kaggle CPU  | 4m 1s    | 186 batches x 32 |\n",
    "| B - Only numpy | Kaggle GPU  | 11s    | 186 batches x 32 |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-21 19:21:38.116318: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import keras\n",
    "import pathlib\n",
    "import os \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import timeit\n",
    "\n",
    "np.random.seed(536)\n",
    "\n",
    "base_dir = pathlib.Path(\"../data/reduced_ds\")\n",
    "path_to_data = pathlib.Path(\"../data/reduced_ds/spectrograms_reduced_800.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded train.csv. Added target column.\n"
     ]
    }
   ],
   "source": [
    "TARGETS = ['seizure_vote', 'lpd_vote', 'gpd_vote', 'lrda_vote', 'grda_vote', 'other_vote']\n",
    "\n",
    "df_traincsv = pd.read_csv(f'{base_dir}/train.csv')\n",
    "df_traincsv.loc[df_traincsv.expert_consensus == 'Seizure', 'target'] = 0\n",
    "df_traincsv.loc[df_traincsv.expert_consensus == 'LPD', 'target'] = 1\n",
    "df_traincsv.loc[df_traincsv.expert_consensus == 'GPD', 'target'] = 2\n",
    "df_traincsv.loc[df_traincsv.expert_consensus == 'LRDA', 'target'] = 3\n",
    "df_traincsv.loc[df_traincsv.expert_consensus == 'GRDA', 'target'] = 4\n",
    "df_traincsv.loc[df_traincsv.expert_consensus == 'Other', 'target'] = 5\n",
    "\n",
    "print(\"Loaded train.csv. Added target column.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 6443\n",
      "Validation samples: 1611\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Train/Validation indexes in df_traincsv\n",
    "#\n",
    "\n",
    "ptrain = 0.8\n",
    "\n",
    "n_total_samples = df_traincsv.shape[0]\n",
    "cut = int(ptrain*n_total_samples)\n",
    "idx = np.random.permutation(n_total_samples)\n",
    "idx_train = idx[0:cut]\n",
    "idx_val = idx[cut:]\n",
    "print(\"Train samples:\", len(idx_train))\n",
    "print(\"Validation samples:\", len(idx_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model definition\n",
    "\n",
    "The same for all tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def make_model(input_shape, num_classes):\n",
    "    input_layer = keras.layers.Input(input_shape)\n",
    "\n",
    "    #max1 = keras.layers.MaxPooling1D(pool_size=2)(input_layer)\n",
    "    \n",
    "    conv1 = keras.layers.Conv2D(filters=32, kernel_size=3, padding=\"same\")(input_layer)\n",
    "    #conv1 = keras.layers.BatchNormalization()(conv1)\n",
    "    conv1 = keras.layers.MaxPooling2D(pool_size=8)(conv1)\n",
    "    conv1 = keras.layers.ReLU()(conv1)\n",
    "    \n",
    "    conv2 = keras.layers.Conv2D(filters=64, kernel_size=7, padding=\"same\")(conv1)\n",
    "    #conv2 = keras.layers.BatchNormalization()(conv2)\n",
    "    conv2 = keras.layers.MaxPooling2D(pool_size=8)(conv2)\n",
    "    conv2 = keras.layers.ReLU()(conv2)\n",
    "\n",
    "    conv3 = keras.layers.Conv2D(filters=256, kernel_size=7, padding=\"same\")(conv2)\n",
    "    #conv3 = keras.layers.BatchNormalization()(conv3)\n",
    "    conv3 = keras.layers.MaxPooling2D(pool_size=2)(conv3)\n",
    "    conv3 = keras.layers.ReLU()(conv3)\n",
    "\n",
    "    # conv4 = keras.layers.Conv1D(filters=512, kernel_size=3, padding=\"same\")(conv3)\n",
    "    # conv4 = keras.layers.BatchNormalization()(conv4)\n",
    "    # conv4 = keras.layers.MaxPooling1D(pool_size=4)(conv4)\n",
    "    # conv4 = keras.layers.ReLU()(conv4)\n",
    "\n",
    "    fltn  = keras.layers.Flatten()(conv3) \n",
    "    \n",
    "    relu1 = keras.layers.Dense(256)(fltn)\n",
    "    relu1 = keras.layers.ReLU()(relu1)\n",
    "\n",
    "    relu2 = keras.layers.Dense(64)(relu1)\n",
    "    relu2 = keras.layers.ReLU(64)(relu2)\n",
    "\n",
    "    lin = keras.layers.Dense(2)(relu2)\n",
    "\n",
    "    output_layer = keras.layers.Dense(num_classes, activation=\"softmax\")(lin)\n",
    "\n",
    "    return keras.models.Model(inputs=input_layer, outputs=output_layer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A - Same as first try\n",
    "\n",
    "Using pandas to access the info."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Data generator\n",
    "#\n",
    "\n",
    "class DataGenerator(keras.utils.Sequence):\n",
    "    'Generates data for Keras'\n",
    "    def __init__(self, items, path_to_data, batch_size=32, dim=(300,400), n_channels=1,\n",
    "                 n_classes=6, shuffle=True):\n",
    "        'Initialization'\n",
    "        sel = [(\"spectrogram_id\", \"in\", items['spectrogram_id'])]\n",
    "        self.data = pd.read_parquet(path_to_data, filters=sel)\n",
    "        self.data.replace(np.nan, 0, inplace=True)\n",
    "        self.columns = self.data.columns[2:]\n",
    "        self.dim = dim\n",
    "        self.batch_size = batch_size\n",
    "        # self.labels = labels\n",
    "        self.items = items\n",
    "        self.len = items.shape[0]\n",
    "        self.n_channels = n_channels\n",
    "        self.n_classes = n_classes\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.ceil(self.len / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "\n",
    "        # Find list of IDs\n",
    "        # items_temp = self.items.iloc[indexes]\n",
    "\n",
    "        # Generate data\n",
    "        X, y = self.__data_generation(indexes)\n",
    "\n",
    "        return X, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.indexes = np.arange(self.len)\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __data_generation(self, indexes):\n",
    "        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
    "        # Initialization\n",
    "        true_size = len(indexes)\n",
    "        X = np.empty((true_size, *self.dim, self.n_channels))\n",
    "        y = np.empty((true_size), dtype=int)\n",
    "\n",
    "        # Generate data\n",
    "        for i, idx in enumerate(indexes):\n",
    "            item = self.items.iloc[idx]\n",
    "            # Store sample\n",
    "            X[i,] = self.data.loc[(self.data.spectrogram_id == int(item.spectrogram_id))&\n",
    "               (self.data.time >= item.spectrogram_label_offset_seconds)&\n",
    "               (self.data.time < item.spectrogram_label_offset_seconds + 600)][self.columns].to_numpy(copy=True).reshape((*self.dim,1))\n",
    "\n",
    "            # Store class\n",
    "            y[i] = int(item.target)\n",
    "\n",
    "        return X, keras.utils.to_categorical(y, num_classes=self.n_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing this class, try to measure performance of loading batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.23 s, sys: 3.05 s, total: 6.28 s\n",
      "Wall time: 1.01 s\n"
     ]
    }
   ],
   "source": [
    "items = df_traincsv[['spectrogram_id','spectrogram_label_offset_seconds',\n",
    "                      'target']].iloc[idx_train].reset_index(drop=True)\n",
    "\n",
    "# Parameters\n",
    "params = {\n",
    "    'dim': (300,400),\n",
    "    'batch_size': 32,\n",
    "    'n_classes': 6,\n",
    "    'n_channels': 1,\n",
    "    'shuffle': True\n",
    "    }\n",
    "\n",
    "%time training_generator = DataGenerator(items, path_to_data, **params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "202"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_generator.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 75.5 ms, sys: 3.05 ms, total: 78.6 ms\n",
      "Wall time: 77.7 ms\n"
     ]
    }
   ],
   "source": [
    "%time a = training_generator.__getitem__(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.48 s, sys: 2.99 s, total: 6.47 s\n",
      "Wall time: 882 ms\n",
      "CPU times: user 3.1 s, sys: 2.07 s, total: 5.17 s\n",
      "Wall time: 662 ms\n",
      "202/202 [==============================] - 109s 535ms/step - loss: 3.4265 - accuracy: 0.1842 - val_loss: 1.8924 - val_accuracy: 0.1539\n",
      "CPU times: user 20min 58s, sys: 37.5 s, total: 21min 36s\n",
      "Wall time: 1min 48s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7fb0d4728190>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "items_train = df_traincsv[['spectrogram_id','spectrogram_label_offset_seconds',\n",
    "                    'target']].iloc[idx_train].reset_index(drop=True)\n",
    "items_val = df_traincsv[['spectrogram_id','spectrogram_label_offset_seconds',\n",
    "                    'target']].iloc[idx_val].reset_index(drop=True)\n",
    "\n",
    "# Parameters\n",
    "params = {\n",
    "    'dim': (300,400),\n",
    "    'batch_size': 32,\n",
    "    'n_classes': 6,\n",
    "    'n_channels': 1,\n",
    "    'shuffle': True\n",
    "    }\n",
    "\n",
    "%time training_generator = DataGenerator(items_train, path_to_data, **params)\n",
    "%time validation_generator = DataGenerator(items_val, path_to_data, **params)\n",
    "\n",
    "\n",
    "model = make_model(input_shape=(*params['dim'],1), num_classes=6)\n",
    "model.compile(loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "%time model.fit(training_generator, epochs=1, validation_data=validation_generator)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B - Avoiding Pandas when using tensorflow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numpy arrays are passed by reference.\n",
    "\n",
    "Except when you operate on the right hand side: b = a + 1, b is a copy.\n",
    "\n",
    "Example, in an object of this class,\"self.a\" is a reference to \"a\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4 5]\n",
      "[0 1 2 3 4 5]\n",
      "[ 0  1 44  3  4  5]\n",
      "[ 0  1 44  3  4  5]\n"
     ]
    }
   ],
   "source": [
    "class arr():\n",
    "    def __init__(self, a):\n",
    "        self.a = a\n",
    "\n",
    "    def dis(self):\n",
    "        print(self.a)\n",
    "\n",
    "a = np.array([0,1,2,3,4,5])\n",
    "\n",
    "print(a)\n",
    "b = arr(a)\n",
    "b.dis()\n",
    "a[2] = 44\n",
    "print(a)\n",
    "b.dis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Data generator using numpy and no pandas.\n",
    "#\n",
    "\n",
    "class DataGenerator(keras.utils.Sequence):\n",
    "    'Generates data for Keras'\n",
    "    def __init__(self, items, data, batch_size=32, dim=(300,400), n_channels=1,\n",
    "                 n_classes=6, shuffle=True):\n",
    "        ''' Initialization\n",
    "        items: [eeg_id, eeg_sub_id, idx of offset, target]\n",
    "        '''\n",
    "        self.data = data\n",
    "        self.items = items\n",
    "        self.dim = dim\n",
    "        self.batch_size = batch_size\n",
    "        self.len = items.shape[0]\n",
    "        self.n_channels = n_channels\n",
    "        self.n_classes = n_classes\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.ceil(self.len / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "\n",
    "        # Generate data\n",
    "        X, y = self.__data_generation(indexes)\n",
    "\n",
    "        return X, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.indexes = np.arange(self.len)\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __data_generation(self, indexes):\n",
    "        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
    "        # Initialization\n",
    "        true_size = len(indexes)\n",
    "        X = np.empty((true_size, *self.dim, self.n_channels))\n",
    "        y = np.empty((true_size), dtype=int)\n",
    "\n",
    "        # Generate data\n",
    "        for i, idx in enumerate(indexes):\n",
    "            item = self.items[idx]\n",
    "            # print(item)  # Uncomment for testing.\n",
    "            # Store sample\n",
    "            X[i,] = self.data[item[2]:(item[2]+300)].reshape(*self.dim,1)\n",
    "            # Store class\n",
    "            y[i] = item[3]\n",
    "\n",
    "        return X, keras.utils.to_categorical(y, num_classes=self.n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 5933\n",
      "Validation samples: 1484\n"
     ]
    }
   ],
   "source": [
    "path_to_data = \"../data/00_spectrograms_reduced.npy\"\n",
    "path_to_items = \"../data/00_sub_spectrograms_idxs_reduced.npy\"\n",
    "\n",
    "data = np.load(path_to_data)\n",
    "items = np.load(path_to_items)\n",
    "n_total_samples = items.shape[0]\n",
    "\n",
    "ptrain = 0.8\n",
    "items = np.random.permutation(items)\n",
    "\n",
    "cut = int(ptrain*n_total_samples)\n",
    "items_train = items[0:cut]\n",
    "items_val = items[cut:]\n",
    "print(\"Train samples:\", len(items_train))\n",
    "print(\"Validation samples:\", len(items_val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 154 µs, sys: 26 µs, total: 180 µs\n",
      "Wall time: 170 µs\n",
      "CPU times: user 46 µs, sys: 8 µs, total: 54 µs\n",
      "Wall time: 52 µs\n"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "params = {\n",
    "    'dim': (300,400),\n",
    "    'batch_size': 32,\n",
    "    'n_classes': 6,\n",
    "    'n_channels': 1,\n",
    "    'shuffle': True\n",
    "    }\n",
    "\n",
    "%time training_generator = DataGenerator(items_train, data, **params)\n",
    "%time validation_generator = DataGenerator(items_val, data, **params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing this new class. Check if the items loaded in the batches are equal to the original spectrograms.\n",
    "\n",
    "PASSED."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 300, 400, 1)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x,y = training_generator.__getitem__(0)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded train.csv. Added target column.\n"
     ]
    }
   ],
   "source": [
    "TARGETS = ['seizure_vote', 'lpd_vote', 'gpd_vote', 'lrda_vote', 'grda_vote', 'other_vote']\n",
    "\n",
    "kaggle_data = \"../../data/hms\"\n",
    "df_traincsv = pd.read_csv(f'{kaggle_data}/train.csv')\n",
    "df_traincsv.loc[df_traincsv.expert_consensus == 'Seizure', 'target'] = 0\n",
    "df_traincsv.loc[df_traincsv.expert_consensus == 'LPD', 'target'] = 1\n",
    "df_traincsv.loc[df_traincsv.expert_consensus == 'GPD', 'target'] = 2\n",
    "df_traincsv.loc[df_traincsv.expert_consensus == 'LRDA', 'target'] = 3\n",
    "df_traincsv.loc[df_traincsv.expert_consensus == 'GRDA', 'target'] = 4\n",
    "df_traincsv.loc[df_traincsv.expert_consensus == 'Other', 'target'] = 5\n",
    "\n",
    "print(\"Loaded train.csv. Added target column.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Columns of interest.\n",
    "df = pd.read_parquet(f'{kaggle_data}/train_spectrograms/399182714.parquet')\n",
    "columns = df.columns[1:]\n",
    "\n",
    "eeg_id = 1460778765\n",
    "eeg_sub_id = 168\n",
    "i = 5\n",
    "\n",
    "# spectrogram_id = 0\n",
    "# for i in np.arange(x.shape[0]):\n",
    "item_train = df_traincsv.loc[(df_traincsv.eeg_id == eeg_id)&(df_traincsv.eeg_sub_id == eeg_sub_id)].iloc[0]\n",
    "    # if item_train.spectrogram_id != spectrogram_id:\n",
    "    #     spectrogram_id = item_train.spectrogram_id\n",
    "    #     df = pd.read_parquet(f'{base_dir}/train_spectrograms/{spectrogram_id}.parquet')\n",
    "    #     df.replace(np.nan, 0, inplace=True)\n",
    "\n",
    "spectrogram_id = item_train.spectrogram_id\n",
    "df = pd.read_parquet(f'{kaggle_data}/train_spectrograms/{spectrogram_id}.parquet')\n",
    "df.replace(np.nan, 0, inplace=True)\n",
    "\n",
    "offset = item_train.spectrogram_label_offset_seconds\n",
    "original_spec = df.loc[(df.time >= offset)&(df.time < (offset + 600))][columns].to_numpy(copy=True)\n",
    "# idx = items[i][2]\n",
    "saved_spec = x[i,:,:,0]\n",
    "np.all(saved_spec == original_spec)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "186/186 [==============================] - 196s 1s/step - loss: 3.6494 - accuracy: 0.2575 - val_loss: 2.6815 - val_accuracy: 0.2392\n",
      "CPU times: user 7min 55s, sys: 25.1 s, total: 8min 20s\n",
      "Wall time: 3min 16s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7fbeb066bca0>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model = make_model(input_shape=(*params['dim'],1), num_classes=6)\n",
    "model.compile(loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "%time model.fit(training_generator, epochs=1, validation_data=validation_generator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gsdc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
