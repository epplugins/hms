{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing speed.\n",
    "\n",
    "Load single file dataset:\n",
    "\n",
    "HMS single file spectrograms numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import regularizers\n",
    "\n",
    "base_dir = pathlib.Path(\"/kaggle/input/hms-harmful-brain-activity-classification\")\n",
    "\n",
    "df_traincsv = pd.read_csv(f'{base_dir}/train.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_data = \"/kaggle/input/hms-single-numpy/00_single_spectrograms_originals_numpy.npy\"\n",
    "path_to_items = \"/kaggle/input/hms-single-numpy/00_single_spectrograms_originals_numpy_items.npy\"\n",
    "\n",
    "data = np.load(path_to_data)\n",
    "items = np.load(path_to_items)\n",
    "n_total_samples = items.shape[0]\n",
    "\n",
    "ptrain = 0.8\n",
    "\n",
    "idx_train = np.array([], dtype=int)\n",
    "idx_val = np.array([], dtype=int)\n",
    "for i in np.arange(6):\n",
    "    idx = np.where(items[:,3] == i)[0]\n",
    "    idx = np.random.permutation(idx)\n",
    "    cut = int(ptrain*idx.shape[0])\n",
    "    idx_train = np.append(idx_train, idx[0:cut])\n",
    "    idx_val = np.append(idx_val, idx[cut:])\n",
    "\n",
    "items_train = items[idx_train]\n",
    "items_val = items[idx_val]\n",
    "print(\"Train samples:\", len(items_train))\n",
    "print(\"Validation samples:\", len(items_val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Data generator using numpy and no pandas.\n",
    "#\n",
    "# Original spectrograms.\n",
    "# 50 seconds slice\n",
    "# 4 channels\n",
    "#\n",
    "\n",
    "class DataGenerator(keras.utils.Sequence):\n",
    "    'Generates data for Keras'\n",
    "    def __init__(self, items, data, batch_size=32, n_classes=6, shuffle=True):\n",
    "        ''' Initialization\n",
    "        items: [eeg_id, eeg_sub_id, idx of offset, target]\n",
    "        '''\n",
    "        self.n_channels = 4\n",
    "        self.n_freqs = 40\n",
    "        self.dim = (25, self.n_freqs)\n",
    "\n",
    "        self.data = data\n",
    "        self.items = items\n",
    "        self.batch_size = batch_size\n",
    "        self.len = items.shape[0]\n",
    "        self.n_classes = n_classes\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.ceil(self.len / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "\n",
    "        # Generate data\n",
    "        X, y = self.__data_generation(indexes)\n",
    "\n",
    "        return X, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.indexes = np.arange(self.len)\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __data_generation(self, indexes):\n",
    "        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
    "        # Initialization\n",
    "        true_size = len(indexes)\n",
    "        X = np.empty((true_size, *self.dim, self.n_channels))\n",
    "        y = np.empty((true_size), dtype=int)\n",
    "\n",
    "        # Generate data\n",
    "        for i, idx in enumerate(indexes):\n",
    "            item = self.items[idx]\n",
    "            # print(item)  # Uncomment for testing.\n",
    "            # Sample is 50 second long, that's 25 rows.\n",
    "            initial = item[2] + 137\n",
    "            final = initial + 25\n",
    "            for c in np.arange(self.n_channels):\n",
    "                cinitial = c * 100\n",
    "                cfinal = cinitial + self.n_freqs\n",
    "                X[i,:,:,c] = self.data[initial:final, cinitial:cfinal]\n",
    "            # Store class\n",
    "            y[i] = item[3]\n",
    "\n",
    "        return X, keras.utils.to_categorical(y, num_classes=self.n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def make_model(input_shape, num_classes):\n",
    "    input_layer = keras.layers.Input(input_shape)\n",
    "\n",
    "    conv1 = keras.layers.Conv2D(filters=32,\n",
    "                                kernel_size=3,\n",
    "                                strides=1,\n",
    "                                padding=\"same\",\n",
    "                                data_format=\"channels_last\",\n",
    "                                # kernel_regularizer=regularizers.l2(0.001),\n",
    "                                # use_bias=True,\n",
    "                                )(input_layer)\n",
    "    conv1 = keras.layers.BatchNormalization()(conv1)\n",
    "    # conv1 = keras.layers.MaxPooling2D(pool_size=8)(conv1)\n",
    "    conv1 = keras.layers.ReLU()(conv1)\n",
    "    \n",
    "    conv2 = keras.layers.Conv2D(filters=32,\n",
    "                                kernel_size=5,\n",
    "                                padding=\"same\",\n",
    "                                data_format=\"channels_last\",\n",
    "                                )(conv1)\n",
    "    #conv2 = keras.layers.BatchNormalization()(conv2)\n",
    "    # conv2 = keras.layers.MaxPooling2D(pool_size=8)(conv2)\n",
    "    conv2 = keras.layers.ReLU()(conv2)\n",
    "\n",
    "    conv3 = keras.layers.Conv2D(filters=64,\n",
    "                                kernel_size=7,\n",
    "                                padding=\"same\",\n",
    "                                data_format=\"channels_last\",\n",
    "                                )(conv2)\n",
    "    conv3 = keras.layers.ReLU()(conv3)\n",
    "    # conv3 = keras.layers.MaxPooling2D(pool_size=2)(conv3)\n",
    "\n",
    "    conv4 = keras.layers.Conv2D(filters=64,\n",
    "                                kernel_size=3,\n",
    "                                padding=\"same\",\n",
    "                                data_format=\"channels_last\",\n",
    "                                )(conv3)\n",
    "    # conv4 = keras.layers.BatchNormalization()(conv4)\n",
    "    conv4 = keras.layers.ReLU()(conv4)\n",
    "    conv4 = keras.layers.MaxPooling2D(pool_size=2)(conv4)\n",
    "\n",
    "    fltn  = keras.layers.Flatten()(conv4) \n",
    "    dense1 = keras.layers.Dense(256)(fltn)\n",
    "    # dout1 = keras.layers.Dropout(rate=0.4)(dense1)\n",
    "    \n",
    "    # relu1 = keras.layers.Dense(256)(fltn)\n",
    "    # relu1 = keras.layers.ReLU()(relu1)\n",
    "\n",
    "    # relu2 = keras.layers.Dense(64)(relu1)\n",
    "    # relu2 = keras.layers.ReLU(64)(relu2)\n",
    "\n",
    "#     lin = keras.layers.Dense(2)(relu2)\n",
    "\n",
    "    output_layer = keras.layers.Dense(num_classes, activation=\"softmax\")(dense1)\n",
    "\n",
    "    return keras.models.Model(inputs=input_layer, outputs=output_layer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "params = {\n",
    "    'batch_size': 32,\n",
    "    'n_classes': 6,\n",
    "    'shuffle': True\n",
    "    }\n",
    "\n",
    "training_generator = DataGenerator(items_train, data, **params)\n",
    "validation_generator = DataGenerator(items_val, data, **params)\n",
    "\n",
    "model = make_model(input_shape=(25,40,4), num_classes=6)\n",
    "model.compile(optimizer='sgd',\n",
    "            loss='categorical_crossentropy',\n",
    "            metrics=[tf.keras.metrics.CategoricalCrossentropy()])\n",
    "\n",
    "model.fit(training_generator, epochs=7, validation_data=validation_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGETS = ['seizure_vote', 'lpd_vote', 'gpd_vote', 'lrda_vote', 'grda_vote', 'other_vote']\n",
    "\n",
    "#\n",
    "# Test Data generator: for predicting.\n",
    "#\n",
    "\n",
    "class test_DataGenerator(keras.utils.Sequence):\n",
    "    'Generates data for Keras'\n",
    "    def __init__(self, ids, path_to_test_data, batch_size=32, n_classes=6):\n",
    "        'Initialization'\n",
    "        self.n_channels = 4\n",
    "        self.dim = (25,40)\n",
    "        self.n_freqs = 40\n",
    "\n",
    "        self.path = path_to_test_data\n",
    "        # self.files = os.listdir(path_to_test_data)\n",
    "        self.ids = ids\n",
    "        self.indexes = np.arange(len(self.ids))\n",
    "        # self.columns = self.data.columns[2:]\n",
    "        self.batch_size = batch_size\n",
    "        self.n_classes = n_classes\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.ceil(len(self.ids) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "\n",
    "        # Find list of IDs\n",
    "        # items_temp = self.items.iloc[indexes]\n",
    "\n",
    "        # Generate data\n",
    "        X = self.__data_generation(indexes)\n",
    "\n",
    "        return X\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        # self.indexes = np.arange(self.len)\n",
    "        # if self.shuffle == True:\n",
    "        #     np.random.shuffle(self.indexes)\n",
    "        pass\n",
    "\n",
    "    def __data_generation(self, indexes):\n",
    "        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
    "        # Initialization\n",
    "        X = np.empty((len(indexes), *self.dim, self.n_channels))\n",
    "\n",
    "        # Generate data\n",
    "        for i, idx in enumerate(indexes):\n",
    "            # item = self.items.iloc[idx]\n",
    "            test_spectrogram = pd.read_parquet(f'{self.path}{self.ids[idx]}.parquet')\n",
    "            test_spectrogram.replace(np.nan, 0, inplace=True)\n",
    "\n",
    "            initial = 137\n",
    "            final = initial + 25\n",
    "            for c in np.arange(self.n_channels):\n",
    "                cinitial = c * 100 + 1\n",
    "                cfinal = cinitial + self.n_freqs\n",
    "                # X[i,:,:,c] = self.data[initial:final, cinitial:cfinal]\n",
    "                X[i,:,:,c] = test_spectrogram.iloc[initial:final,cinitial:cfinal].to_numpy(copy=True)\n",
    "\n",
    "        return X\n",
    "\n",
    "\n",
    "# Parameters\n",
    "params = {\n",
    "    'batch_size': 32,\n",
    "    'n_classes': 6,\n",
    "    }\n",
    "\n",
    "\n",
    "base_dir = \"../toy_data\"\n",
    "\n",
    "path_to_test_data = f'{base_dir}/test_spectrograms/'\n",
    "test = pd.read_csv(f'{base_dir}/test.csv')\n",
    "ids = test['spectrogram_id'].values\n",
    "\n",
    "test_generator = test_DataGenerator(ids, path_to_test_data, **params)\n",
    "\n",
    "y_pred = model.predict(test_generator)\n",
    "\n",
    "sub = pd.DataFrame({'eeg_id':test.eeg_id.values})\n",
    "sub[TARGETS] = np.round(y_pred,6)\n",
    "# sub.to_csv('submission.csv',index=False)\n",
    "sub"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
