{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00db8aa9",
   "metadata": {
    "papermill": {
     "duration": 0.004896,
     "end_time": "2024-02-19T18:30:53.912637",
     "exception": false,
     "start_time": "2024-02-19T18:30:53.907741",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# CWT scalograms\n",
    "\n",
    "5 channels (LT, RT, LP, RP, C).\n",
    "\n",
    "Implementing tf.keras.metrics.KLDivergence().\n",
    "\n",
    "- Definitions for scoring.\n",
    "- Training run.\n",
    "- Scoring locally.\n",
    "- Submit for LB scoring.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "91ea511e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-19T18:30:53.923784Z",
     "iopub.status.busy": "2024-02-19T18:30:53.923433Z",
     "iopub.status.idle": "2024-02-19T18:31:06.918431Z",
     "shell.execute_reply": "2024-02-19T18:31:06.917331Z"
    },
    "papermill": {
     "duration": 13.003552,
     "end_time": "2024-02-19T18:31:06.921234",
     "exception": false,
     "start_time": "2024-02-19T18:30:53.917682",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import regularizers\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "base_dir = '../../kaggle_data/hms'\n",
    "# base_dir = '../../data/hms'\n",
    "# base_dir = '/kaggle/input/hms-harmful-brain-activity-classification'\n",
    "\n",
    "devset_dir = '../data'\n",
    "# devset_dir = '/kaggle/input/hms-single-spectrograms-v1'\n",
    "\n",
    "path_train = f'{devset_dir}/05_single_cwt_v1_train.npy'\n",
    "path_train_items = f'{devset_dir}/05_single_cwt_v1_train_items.npy'\n",
    "path_val = f'{devset_dir}/05_single_cwt_v1_val.npy'\n",
    "path_val_items = f'{devset_dir}/05_single_cwt_v1_val_items.npy'\n",
    "path_test = f'{devset_dir}/05_single_cwt_v1_test.npy'\n",
    "path_test_items = f'{devset_dir}/05_single_cwt_v1_test_items.npy'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d23428eb",
   "metadata": {
    "papermill": {
     "duration": 0.005501,
     "end_time": "2024-02-19T18:31:06.932688",
     "exception": false,
     "start_time": "2024-02-19T18:31:06.927187",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Definitions\n",
    "\n",
    "For scoring without submitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "71ff0a87",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-19T18:31:06.946175Z",
     "iopub.status.busy": "2024-02-19T18:31:06.945553Z",
     "iopub.status.idle": "2024-02-19T18:31:06.973750Z",
     "shell.execute_reply": "2024-02-19T18:31:06.972889Z"
    },
    "papermill": {
     "duration": 0.037218,
     "end_time": "2024-02-19T18:31:06.975770",
     "exception": false,
     "start_time": "2024-02-19T18:31:06.938552",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas.api.types\n",
    "from typing import Union\n",
    "from typing import Optional\n",
    "\n",
    "\n",
    "class ParticipantVisibleError(Exception):\n",
    "    pass\n",
    "\n",
    "\n",
    "class HostVisibleError(Exception):\n",
    "    pass\n",
    "\n",
    "\n",
    "def treat_as_participant_error(error_message: str, solution: Union[pd.DataFrame, np.ndarray]) -> bool:\n",
    "    ''' Many metrics can raise more errors than can be handled manually. This function attempts\n",
    "    to identify errors that can be treated as ParticipantVisibleError without leaking any competition data.\n",
    "\n",
    "    If the solution is purely numeric, and there are no numbers in the error message,\n",
    "    then the error message is sufficiently unlikely to leak usable data and can be shown to participants.\n",
    "\n",
    "    We expect this filter to reject many safe messages. It's intended only to reduce the number of errors we need to manage manually.\n",
    "    '''\n",
    "    # This check treats bools as numeric\n",
    "    if isinstance(solution, pd.DataFrame):\n",
    "        solution_is_all_numeric = all([pandas.api.types.is_numeric_dtype(x) for x in solution.dtypes.values])\n",
    "        solution_has_bools = any([pandas.api.types.is_bool_dtype(x) for x in solution.dtypes.values])\n",
    "    elif isinstance(solution, np.ndarray):\n",
    "        solution_is_all_numeric = pandas.api.types.is_numeric_dtype(solution)\n",
    "        solution_has_bools = pandas.api.types.is_bool_dtype(solution)\n",
    "\n",
    "    if not solution_is_all_numeric:\n",
    "        return False\n",
    "\n",
    "    for char in error_message:\n",
    "        if char.isnumeric():\n",
    "            return False\n",
    "    if solution_has_bools:\n",
    "        if 'true' in error_message.lower() or 'false' in error_message.lower():\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def safe_call_score(metric_function, solution, submission, **metric_func_kwargs):\n",
    "    '''\n",
    "    Call score. If that raises an error and that already been specifically handled, just raise it.\n",
    "    Otherwise make a conservative attempt to identify potential participant visible errors.\n",
    "    '''\n",
    "    try:\n",
    "        score_result = metric_function(solution, submission, **metric_func_kwargs)\n",
    "    except Exception as err:\n",
    "        error_message = str(err)\n",
    "        if err.__class__.__name__ == 'ParticipantVisibleError':\n",
    "            raise ParticipantVisibleError(error_message)\n",
    "        elif err.__class__.__name__ == 'HostVisibleError':\n",
    "            raise HostVisibleError(error_message)\n",
    "        else:\n",
    "            if treat_as_participant_error(error_message, solution):\n",
    "                raise ParticipantVisibleError(error_message)\n",
    "            else:\n",
    "                raise err\n",
    "    return score_result\n",
    "\n",
    "\n",
    "def verify_valid_probabilities(df: pd.DataFrame, df_name: str):\n",
    "    \"\"\" Verify that the dataframe contains valid probabilities.\n",
    "\n",
    "    The dataframe must be limited to the target columns; do not pass in any ID columns.\n",
    "    \"\"\"\n",
    "    if not pandas.api.types.is_numeric_dtype(df.values):\n",
    "        raise ParticipantVisibleError(f'All target values in {df_name} must be numeric')\n",
    "\n",
    "    if df.min().min() < 0:\n",
    "        raise ParticipantVisibleError(f'All target values in {df_name} must be at least zero')\n",
    "\n",
    "    if df.max().max() > 1:\n",
    "        raise ParticipantVisibleError(f'All target values in {df_name} must be no greater than one')\n",
    "\n",
    "    if not np.allclose(df.sum(axis=1), 1):\n",
    "        raise ParticipantVisibleError(f'Target values in {df_name} do not add to one within all rows')\n",
    "\n",
    "\n",
    "def kl_divergence(solution: pd.DataFrame, submission: pd.DataFrame, epsilon: float, micro_average: bool, sample_weights: Optional[pd.Series]):\n",
    "    # Overwrite solution for convenience\n",
    "    for col in solution.columns:\n",
    "        # Prevent issue with populating int columns with floats\n",
    "        if not pandas.api.types.is_float_dtype(solution[col]):\n",
    "            solution[col] = solution[col].astype(float)\n",
    "\n",
    "        # Clip both the min and max following Kaggle conventions for related metrics like log loss\n",
    "        # Clipping the max avoids cases where the loss would be infinite or undefined, clipping the min\n",
    "        # prevents users from playing games with the 20th decimal place of predictions.\n",
    "        submission[col] = np.clip(submission[col], epsilon, 1 - epsilon)\n",
    "\n",
    "        y_nonzero_indices = solution[col] != 0\n",
    "        solution[col] = solution[col].astype(float)\n",
    "        solution.loc[y_nonzero_indices, col] = solution.loc[y_nonzero_indices, col] * np.log(solution.loc[y_nonzero_indices, col] / submission.loc[y_nonzero_indices, col])\n",
    "        # Set the loss equal to zero where y_true equals zero following the scipy convention:\n",
    "        # https://docs.scipy.org/doc/scipy/reference/generated/scipy.special.rel_entr.html#scipy.special.rel_entr\n",
    "        solution.loc[~y_nonzero_indices, col] = 0\n",
    "\n",
    "    if micro_average:\n",
    "        return np.average(solution.sum(axis=1), weights=sample_weights)\n",
    "    else:\n",
    "        return np.average(solution.mean())\n",
    "\n",
    "def score(\n",
    "        solution: pd.DataFrame,\n",
    "        submission: pd.DataFrame,\n",
    "        row_id_column_name: str,\n",
    "        epsilon: float=10**-15,\n",
    "        micro_average: bool=True,\n",
    "        sample_weights_column_name: Optional[str]=None\n",
    "    ) -> float:\n",
    "    ''' The Kullback-Leibler divergence.\n",
    "    The KL divergence is technically undefined/infinite where the target equals zero.\n",
    "\n",
    "    This implementation always assigns those cases a score of zero; effectively removing them from consideration.\n",
    "    The predictions in each row must add to one so any probability assigned to a case where y == 0 reduces\n",
    "    another prediction where y > 0, so crucially there is an important indirect effect.\n",
    "\n",
    "    https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence\n",
    "\n",
    "    solution: pd.DataFrame\n",
    "    submission: pd.DataFrame\n",
    "    epsilon: KL divergence is undefined for p=0 or p=1. If epsilon is not null, solution and submission probabilities are clipped to max(eps, min(1 - eps, p).\n",
    "    row_id_column_name: str\n",
    "    micro_average: bool. Row-wise average if True, column-wise average if False.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> import pandas as pd\n",
    "    >>> row_id_column_name = \"id\"\n",
    "    >>> score(pd.DataFrame({'id': range(4), 'ham': [0, 1, 1, 0], 'spam': [1, 0, 0, 1]}), pd.DataFrame({'id': range(4), 'ham': [.1, .9, .8, .35], 'spam': [.9, .1, .2, .65]}), row_id_column_name=row_id_column_name)\n",
    "    0.216161...\n",
    "    >>> solution = pd.DataFrame({'id': range(3), 'ham': [0, 0.5, 0.5], 'spam': [0.1, 0.5, 0.5], 'other': [0.9, 0, 0]})\n",
    "    >>> submission = pd.DataFrame({'id': range(3), 'ham': [0, 0.5, 0.5], 'spam': [0.1, 0.5, 0.5], 'other': [0.9, 0, 0]})\n",
    "    >>> score(solution, submission, 'id')\n",
    "    0.0\n",
    "    >>> solution = pd.DataFrame({'id': range(3), 'ham': [0, 0.5, 0.5], 'spam': [0.1, 0.5, 0.5], 'other': [0.9, 0, 0]})\n",
    "    >>> submission = pd.DataFrame({'id': range(3), 'ham': [0.2, 0.3, 0.5], 'spam': [0.1, 0.5, 0.5], 'other': [0.7, 0.2, 0]})\n",
    "    >>> score(solution, submission, 'id')\n",
    "    0.160531...\n",
    "    '''\n",
    "    del solution[row_id_column_name]\n",
    "    del submission[row_id_column_name]\n",
    "\n",
    "    sample_weights = None\n",
    "    if sample_weights_column_name:\n",
    "        if sample_weights_column_name not in solution.columns:\n",
    "            raise ParticipantVisibleError(f'{sample_weights_column_name} not found in solution columns')\n",
    "        sample_weights = solution.pop(sample_weights_column_name)\n",
    "\n",
    "    if sample_weights_column_name and not micro_average:\n",
    "        raise ParticipantVisibleError('Sample weights are only valid if `micro_average` is `True`')\n",
    "\n",
    "    for col in solution.columns:\n",
    "        if col not in submission.columns:\n",
    "            raise ParticipantVisibleError(f'Missing submission column {col}')\n",
    "\n",
    "    verify_valid_probabilities(solution, 'solution')\n",
    "    verify_valid_probabilities(submission, 'submission')\n",
    "\n",
    "    return safe_call_score(kl_divergence, solution, submission, epsilon=epsilon, micro_average=micro_average, sample_weights=sample_weights)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "635a67de",
   "metadata": {
    "papermill": {
     "duration": 0.004653,
     "end_time": "2024-02-19T18:31:07.002231",
     "exception": false,
     "start_time": "2024-02-19T18:31:06.997578",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "94c9a9f4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-19T18:31:07.013190Z",
     "iopub.status.busy": "2024-02-19T18:31:07.012628Z",
     "iopub.status.idle": "2024-02-19T18:31:07.025580Z",
     "shell.execute_reply": "2024-02-19T18:31:07.024843Z"
    },
    "papermill": {
     "duration": 0.020601,
     "end_time": "2024-02-19T18:31:07.027552",
     "exception": false,
     "start_time": "2024-02-19T18:31:07.006951",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Data generator using numpy and no pandas.\n",
    "#\n",
    "# scalograms\n",
    "# 30 seconds slice (I think)\n",
    "# 5 channels (LP, RP, LT, RP, C)\n",
    "#\n",
    "\n",
    "class DataGenerator(keras.utils.Sequence):\n",
    "    'Generates data for Keras'\n",
    "    def __init__(self, path_to_items, path_to_data, batch_size=32, n_classes=6, shuffle=True):\n",
    "        ''' Initialization\n",
    "        item: [eeg_id, eeg_sub_id, idx in sgrams (1st index), target,\n",
    "        seizure_vote, lpd_vote, gpd_vote, lrda_vote,\n",
    "        grda_vote, other_vote]\n",
    "        '''\n",
    "        self.n_channels = 5\n",
    "        # self.n_freqs = 40\n",
    "\n",
    "        self.data = np.load(path_to_data)\n",
    "        self.items = np.load(path_to_items)\n",
    "        self.dim = (self.data.shape[1], self.data.shape[2])\n",
    "        self.batch_size = batch_size\n",
    "        self.len = self.data.shape[0]\n",
    "        self.n_classes = n_classes\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.ceil(self.len / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "\n",
    "        # Generate data\n",
    "        X, y = self.__data_generation(indexes)\n",
    "\n",
    "        return X, y\n",
    "\n",
    "    def get_dim(self):\n",
    "        'Dimensions for the input layer.'\n",
    "        return (self.dim[0], self.dim[1], self.n_channels)\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.indexes = np.arange(self.len)\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __data_generation(self, indexes):\n",
    "        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
    "        # Initialization\n",
    "        true_size = len(indexes)\n",
    "        X = np.empty((true_size, *self.dim, self.n_channels))\n",
    "        y = np.empty((true_size, self.n_classes), dtype=float)\n",
    "\n",
    "        # Generate data\n",
    "        for i, idx in enumerate(indexes):\n",
    "            item = self.items[idx]\n",
    "            # print(item)  # Uncomment for testing.\n",
    "            X[i,:,:,:] = self.data[np.int16(item[2]), :, :, :]\n",
    "            # Store solution\n",
    "            y[i,:] = item[-6:]\n",
    "\n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d42b19af",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-19T18:31:07.038776Z",
     "iopub.status.busy": "2024-02-19T18:31:07.038455Z",
     "iopub.status.idle": "2024-02-19T18:31:07.049272Z",
     "shell.execute_reply": "2024-02-19T18:31:07.048499Z"
    },
    "papermill": {
     "duration": 0.018651,
     "end_time": "2024-02-19T18:31:07.051182",
     "exception": false,
     "start_time": "2024-02-19T18:31:07.032531",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def make_model(input_shape, num_classes):\n",
    "    input_layer = keras.layers.Input(input_shape)\n",
    "\n",
    "    #max1 = keras.layers.MaxPooling1D(pool_size=2)(input_layer)\n",
    "    \n",
    "    conv1 = keras.layers.Conv2D(filters=32, kernel_size=3, padding=\"same\")(input_layer)\n",
    "    #conv1 = keras.layers.BatchNormalization()(conv1)\n",
    "    # conv1 = keras.layers.MaxPooling2D(pool_size=8)(conv1)\n",
    "    conv1 = keras.layers.ReLU()(conv1)\n",
    "    \n",
    "    conv2 = keras.layers.Conv2D(filters=64, kernel_size=7, padding=\"same\")(conv1)\n",
    "    #conv2 = keras.layers.BatchNormalization()(conv2)\n",
    "    # conv2 = keras.layers.MaxPooling2D(pool_size=8)(conv2)\n",
    "    conv2 = keras.layers.ReLU()(conv2)\n",
    "\n",
    "    conv3 = keras.layers.Conv2D(filters=256, kernel_size=7, padding=\"same\")(conv2)\n",
    "    #conv3 = keras.layers.BatchNormalization()(conv3)\n",
    "    conv3 = keras.layers.MaxPooling2D(pool_size=2)(conv3)\n",
    "    conv3 = keras.layers.ReLU()(conv3)\n",
    "\n",
    "    conv4 = keras.layers.Conv1D(filters=512, kernel_size=3, padding=\"same\")(conv3)\n",
    "    conv4 = keras.layers.BatchNormalization()(conv4)\n",
    "    conv4 = keras.layers.MaxPooling2D(pool_size=4)(conv4)\n",
    "    conv4 = keras.layers.ReLU()(conv4)\n",
    "\n",
    "    fltn  = keras.layers.Flatten()(conv4) \n",
    "    \n",
    "    relu1 = keras.layers.Dense(256)(fltn)\n",
    "    relu1 = keras.layers.ReLU()(relu1)\n",
    "\n",
    "    relu2 = keras.layers.Dense(64)(relu1)\n",
    "    relu2 = keras.layers.ReLU(64)(relu2)\n",
    "\n",
    "#     lin = keras.layers.Dense(2)(relu2)\n",
    "\n",
    "    output_layer = keras.layers.Dense(num_classes, activation=\"softmax\")(relu2)\n",
    "\n",
    "    return keras.models.Model(inputs=input_layer, outputs=output_layer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e9fa624a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-19T18:31:07.062937Z",
     "iopub.status.busy": "2024-02-19T18:31:07.062324Z",
     "iopub.status.idle": "2024-02-19T18:31:40.020998Z",
     "shell.execute_reply": "2024-02-19T18:31:40.020003Z"
    },
    "papermill": {
     "duration": 32.97052,
     "end_time": "2024-02-19T18:31:40.026924",
     "exception": false,
     "start_time": "2024-02-19T18:31:07.056404",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observations in training set: 8352\n",
      "Observations in validation set: 2208\n"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "params = {\n",
    "    'batch_size': 32,\n",
    "    'n_classes': 6,\n",
    "    'shuffle': True\n",
    "    }\n",
    "\n",
    "training_generator = DataGenerator(path_train_items, path_train , **params)\n",
    "validation_generator = DataGenerator(path_val_items, path_val, **params)\n",
    "\n",
    "print(\"Observations in training set:\", training_generator.__len__()*params['batch_size'])\n",
    "print(\"Observations in validation set:\", validation_generator.__len__()*params['batch_size'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f0687eef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-19T18:31:40.037626Z",
     "iopub.status.busy": "2024-02-19T18:31:40.037311Z",
     "iopub.status.idle": "2024-02-19T18:42:03.226928Z",
     "shell.execute_reply": "2024-02-19T18:42:03.226028Z"
    },
    "papermill": {
     "duration": 623.197335,
     "end_time": "2024-02-19T18:42:03.228965",
     "exception": false,
     "start_time": "2024-02-19T18:31:40.031630",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1708367509.763754      67 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "261/261 [==============================] - 145s 476ms/step - loss: 1.5820 - kullback_leibler_divergence: 1.5820 - val_loss: 1.4328 - val_kullback_leibler_divergence: 1.4328\n",
      "Epoch 2/5\n",
      "261/261 [==============================] - 119s 457ms/step - loss: 1.4482 - kullback_leibler_divergence: 1.4482 - val_loss: 1.4259 - val_kullback_leibler_divergence: 1.4259\n",
      "Epoch 3/5\n",
      "261/261 [==============================] - 119s 457ms/step - loss: 1.4393 - kullback_leibler_divergence: 1.4393 - val_loss: 1.4304 - val_kullback_leibler_divergence: 1.4304\n",
      "Epoch 4/5\n",
      "261/261 [==============================] - 119s 456ms/step - loss: 1.4339 - kullback_leibler_divergence: 1.4339 - val_loss: 1.4260 - val_kullback_leibler_divergence: 1.4260\n",
      "Epoch 5/5\n",
      "261/261 [==============================] - 119s 457ms/step - loss: 1.4321 - kullback_leibler_divergence: 1.4321 - val_loss: 1.4281 - val_kullback_leibler_divergence: 1.4281\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7cbf4c86b670>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dim = training_generator.get_dim()\n",
    "\n",
    "model = make_model(input_shape=dim, num_classes=6)\n",
    "model.compile(optimizer='sgd',\n",
    "            loss=tf.keras.losses.KLDivergence(),\n",
    "            metrics=[tf.keras.metrics.KLDivergence()])\n",
    "\n",
    "model.fit(training_generator, epochs=5, validation_data=validation_generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f24e5b9",
   "metadata": {
    "papermill": {
     "duration": 0.106847,
     "end_time": "2024-02-19T18:42:03.442617",
     "exception": false,
     "start_time": "2024-02-19T18:42:03.335770",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "983ea1f8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-19T18:42:03.659619Z",
     "iopub.status.busy": "2024-02-19T18:42:03.658863Z",
     "iopub.status.idle": "2024-02-19T18:42:17.517770Z",
     "shell.execute_reply": "2024-02-19T18:42:17.516736Z"
    },
    "papermill": {
     "duration": 13.970827,
     "end_time": "2024-02-19T18:42:17.520255",
     "exception": false,
     "start_time": "2024-02-19T18:42:03.549428",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 70\u001b[0m\n\u001b[1;32m     63\u001b[0m params \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m32\u001b[39m,\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_classes\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m6\u001b[39m,\n\u001b[1;32m     66\u001b[0m     }\n\u001b[1;32m     68\u001b[0m test_generator \u001b[38;5;241m=\u001b[39m TestDataGenerator(path_test_items, path_test, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[0;32m---> 70\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39mpredict(test_generator)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "TARGETS = ['seizure_vote', 'lpd_vote', 'gpd_vote', 'lrda_vote', 'grda_vote', 'other_vote']\n",
    "\n",
    "#\n",
    "# Test Data generator: for predicting\n",
    "# using own test set.\n",
    "# (Not for predicting LB)\n",
    "#\n",
    "\n",
    "class TestDataGenerator(keras.utils.Sequence):\n",
    "    'Generates data for Keras'\n",
    "    def __init__(self, path_to_items, path_to_data, batch_size=32, n_classes=6, shuffle=False):\n",
    "        ''' Initialization\n",
    "        items: [eeg_id, eeg_sub_id, idx of offset, target, ...]\n",
    "        '''\n",
    "        self.n_channels = 2\n",
    "        self.data = np.load(path_to_data)\n",
    "        self.items = np.load(path_to_items)\n",
    "        self.dim = (self.data.shape[1], self.data.shape[2])\n",
    "        self.batch_size = batch_size\n",
    "        self.len = self.data.shape[0]\n",
    "        self.n_classes = n_classes\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.ceil(self.len / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "\n",
    "        # Generate data\n",
    "        X = self.__data_generation(indexes)\n",
    "\n",
    "        return X\n",
    "\n",
    "    def get_dim(self):\n",
    "        'Dimensions for the input layer.'\n",
    "        return (self.dim[0], self.dim[1], self.n_channels)\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.indexes = np.arange(self.len)\n",
    "        # pass \n",
    "        \n",
    "    def __data_generation(self, indexes):\n",
    "        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
    "        # Initialization\n",
    "        true_size = len(indexes)\n",
    "        X = np.empty((true_size, *self.dim, self.n_channels))\n",
    "\n",
    "        # Generate data\n",
    "        for i, idx in enumerate(indexes):\n",
    "            item = self.items[idx]\n",
    "            # print(item)  # Uncomment for testing.\n",
    "            X[i,:,:,:] = self.data[np.int16(item[2]), :, :, :]\n",
    "\n",
    "        return X\n",
    "    \n",
    "                \n",
    "params = {\n",
    "    'batch_size': 32,\n",
    "    'n_classes': 6,\n",
    "    }\n",
    "\n",
    "test_generator = TestDataGenerator(path_test_items, path_test, **params)\n",
    "\n",
    "y_pred = model.predict(test_generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "294aab41",
   "metadata": {
    "papermill": {
     "duration": 0.112005,
     "end_time": "2024-02-19T18:42:17.746564",
     "exception": false,
     "start_time": "2024-02-19T18:42:17.634559",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Scoring without submission\n",
    "\n",
    "Using a local test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b22e0b2b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-19T18:42:17.974910Z",
     "iopub.status.busy": "2024-02-19T18:42:17.974572Z",
     "iopub.status.idle": "2024-02-19T18:42:18.044800Z",
     "shell.execute_reply": "2024-02-19T18:42:18.043869Z"
    },
    "papermill": {
     "duration": 0.187177,
     "end_time": "2024-02-19T18:42:18.046703",
     "exception": false,
     "start_time": "2024-02-19T18:42:17.859526",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_26/1435129968.py:95: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  solution[col] = solution[col].astype(float)\n",
      "/tmp/ipykernel_26/1435129968.py:95: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  solution[col] = solution[col].astype(float)\n",
      "/tmp/ipykernel_26/1435129968.py:95: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  solution[col] = solution[col].astype(float)\n",
      "/tmp/ipykernel_26/1435129968.py:95: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  solution[col] = solution[col].astype(float)\n",
      "/tmp/ipykernel_26/1435129968.py:95: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  solution[col] = solution[col].astype(float)\n",
      "/tmp/ipykernel_26/1435129968.py:95: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  solution[col] = solution[col].astype(float)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.503097287866345"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "test_items = np.load(f'{devset_dir}/03_single_spectrograms_v1_test_items.npy')\n",
    "# test_items = np.load(f'{devset_dir}/03_single_spectrograms_reduced_v1_test_items.npy')\n",
    "df_test_items = pd.DataFrame(test_items)\n",
    "df_test_items[0] = df_test_items[0].astype(int)\n",
    "\n",
    "sub = pd.DataFrame({'eeg_id':df_test_items[0]})\n",
    "sub[TARGETS] = np.round(y_pred,6)\n",
    "sub.to_csv('submission.csv',index=False)\n",
    "\n",
    "df_test_scoring = df_test_items[[0,4,5,6,7,8,9]]\n",
    "df_test_scoring.columns = sub.columns\n",
    "df_test_scoring\n",
    "\n",
    "score(df_test_scoring, sub, 'eeg_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "460d921c",
   "metadata": {},
   "source": [
    "# Uniform probabilities classificator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "26232dc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5612/1435129968.py:95: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  solution[col] = solution[col].astype(float)\n",
      "/tmp/ipykernel_5612/1435129968.py:95: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  solution[col] = solution[col].astype(float)\n",
      "/tmp/ipykernel_5612/1435129968.py:95: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  solution[col] = solution[col].astype(float)\n",
      "/tmp/ipykernel_5612/1435129968.py:95: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  solution[col] = solution[col].astype(float)\n",
      "/tmp/ipykernel_5612/1435129968.py:95: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  solution[col] = solution[col].astype(float)\n",
      "/tmp/ipykernel_5612/1435129968.py:95: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  solution[col] = solution[col].astype(float)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.3718073504111403"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_npy = np.load(path_test)\n",
    "test_items = np.load(path_test_items)\n",
    "\n",
    "y_pred = np.ones((test_items.shape[0],6),dtype=float)\n",
    "y_pred[:,0:4] = y_pred[:,0:4] * 0.167\n",
    "y_pred[:,4:] = y_pred[:,4:] * 0.166\n",
    "\n",
    "df_test_items = pd.DataFrame(test_items)\n",
    "df_test_items[0] = df_test_items[0].astype(int)\n",
    "\n",
    "sub = pd.DataFrame({'eeg_id':df_test_items[0]})\n",
    "sub[TARGETS] = np.round(y_pred,6)\n",
    "# sub.to_csv('submission.csv',index=False)\n",
    "\n",
    "df_test_scoring = df_test_items[[0,4,5,6,7,8,9]]\n",
    "df_test_scoring.columns = sub.columns\n",
    "df_test_scoring\n",
    "\n",
    "score(df_test_scoring, sub, 'eeg_id')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c07411b",
   "metadata": {},
   "source": [
    "# Submit to LB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1100d320",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 7469972,
     "sourceId": 59093,
     "sourceType": "competition"
    },
    {
     "datasetId": 4432380,
     "sourceId": 7611741,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4461201,
     "sourceId": 7652435,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30648,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 689.517384,
   "end_time": "2024-02-19T18:42:20.635927",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-02-19T18:30:51.118543",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
